{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":320111,"sourceType":"datasetVersion","datasetId":134715}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Step 1: Download the IMDB Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndataset_path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\ndf = pd.read_csv(dataset_path)\n\n# Display basic information about the dataset\nprint(df.info())\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:16:46.554495Z","iopub.execute_input":"2025-02-04T09:16:46.554736Z","iopub.status.idle":"2025-02-04T09:16:48.823633Z","shell.execute_reply.started":"2025-02-04T09:16:46.554715Z","shell.execute_reply":"2025-02-04T09:16:48.822682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 2: Data Preprocessing","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndataset_path = '/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv'\ndf = pd.read_csv(dataset_path)\n\n# Encode sentiment column\ndf['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n\n# Retain only the review and label columns\ndf = df[['review', 'sentiment']]\n\n# Split the data into training, validation, and testing sets\ntrain_data, temp_data = train_test_split(df, test_size=0.2, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n\n# Display basic information about the dataset\nprint(df.info())\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:17:00.138058Z","iopub.execute_input":"2025-02-04T09:17:00.138326Z","iopub.status.idle":"2025-02-04T09:17:01.303730Z","shell.execute_reply.started":"2025-02-04T09:17:00.138305Z","shell.execute_reply":"2025-02-04T09:17:01.302862Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 3: Model Selection and Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\n# Select pre-trained model and tokenizer\nmodel_name = 'distilbert-base-uncased'\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\ndef tokenize_and_format(examples):\n    tokens = tokenizer(examples['review'], truncation=True, padding='max_length', max_length=256)\n    tokens['labels'] = examples['sentiment']  # Assign labels explicitly\n    return tokens\n\ntrain_dataset = Dataset.from_pandas(train_data).map(tokenize_and_format, batched=True)\nval_dataset = Dataset.from_pandas(val_data).map(tokenize_and_format, batched=True)\ntest_dataset = Dataset.from_pandas(test_data).map(tokenize_and_format, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:17:15.842114Z","iopub.execute_input":"2025-02-04T09:17:15.842392Z","iopub.status.idle":"2025-02-04T09:18:01.247105Z","shell.execute_reply.started":"2025-02-04T09:17:15.842371Z","shell.execute_reply":"2025-02-04T09:18:01.246190Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 4: Fine-Tune the Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\nfrom datasets import Dataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# Load pre-trained model\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=1,\n    report_to='none',\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    evaluation_strategy='epoch',\n    save_strategy='epoch',\n    learning_rate=5e-5,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n# Define evaluation metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    accuracy = accuracy_score(labels, predictions)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:18:14.717133Z","iopub.execute_input":"2025-02-04T09:18:14.717430Z","iopub.status.idle":"2025-02-04T09:18:16.810301Z","shell.execute_reply.started":"2025-02-04T09:18:14.717409Z","shell.execute_reply":"2025-02-04T09:18:16.809402Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fine-tune the model\ntrainer.train()\n\n# Display basic information about the dataset\nprint(df.info())\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:18:37.376368Z","iopub.execute_input":"2025-02-04T09:18:37.376729Z","iopub.status.idle":"2025-02-04T09:27:23.071648Z","shell.execute_reply.started":"2025-02-04T09:18:37.376699Z","shell.execute_reply":"2025-02-04T09:27:23.070766Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 5: Save and Upload the Model to Hugging Face","metadata":{}},{"cell_type":"code","source":"model_save_path = \"/kaggle/working/imdb_distilbert\"\nmodel.save_pretrained(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:28:11.164057Z","iopub.execute_input":"2025-02-04T09:28:11.164354Z","iopub.status.idle":"2025-02-04T09:28:11.822407Z","shell.execute_reply.started":"2025-02-04T09:28:11.164331Z","shell.execute_reply":"2025-02-04T09:28:11.821702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:29:02.835589Z","iopub.execute_input":"2025-02-04T09:29:02.835950Z","iopub.status.idle":"2025-02-04T09:29:06.856569Z","shell.execute_reply.started":"2025-02-04T09:29:02.835922Z","shell.execute_reply":"2025-02-04T09:29:06.855481Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:29:49.663446Z","iopub.execute_input":"2025-02-04T09:29:49.663830Z","iopub.status.idle":"2025-02-04T09:29:49.681708Z","shell.execute_reply.started":"2025-02-04T09:29:49.663799Z","shell.execute_reply":"2025-02-04T09:29:49.680756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import whoami\n\nprint(whoami())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:30:26.218017Z","iopub.execute_input":"2025-02-04T09:30:26.218328Z","iopub.status.idle":"2025-02-04T09:30:26.306105Z","shell.execute_reply.started":"2025-02-04T09:30:26.218306Z","shell.execute_reply":"2025-02-04T09:30:26.305397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\nmodel_save_path = \"/kaggle/working/imdb_distilbert\"  # Your saved model path\nmodel = AutoModelForSequenceClassification.from_pretrained(model_save_path)\ntokenizer = AutoTokenizer.from_pretrained(model_save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:32:44.919820Z","iopub.execute_input":"2025-02-04T09:32:44.920137Z","iopub.status.idle":"2025-02-04T09:32:44.983653Z","shell.execute_reply.started":"2025-02-04T09:32:44.920111Z","shell.execute_reply":"2025-02-04T09:32:44.982760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import HfApi\n\n# Define the repo name (it will be created under your Hugging Face account)\nrepo_name = 'distilbert_IMDB'  # Change this to your preferred name\n\n# Push model and tokenizer to the Hugging Face Hub\nmodel.push_to_hub(repo_name)\ntokenizer.push_to_hub(repo_name)\n\nprint(f\"Model uploaded to: https://huggingface.co/Krati132/{repo_name}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:36:25.230065Z","iopub.execute_input":"2025-02-04T09:36:25.230371Z","iopub.status.idle":"2025-02-04T09:36:27.783958Z","shell.execute_reply.started":"2025-02-04T09:36:25.230348Z","shell.execute_reply":"2025-02-04T09:36:27.783206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Part 2: API Development and Testing","metadata":{}},{"cell_type":"markdown","source":"Step 6: Set Up the Backend API","metadata":{}},{"cell_type":"code","source":"!pip install fastapi uvicorn transformers torch sentencepiece accelerate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:40:34.050876Z","iopub.execute_input":"2025-02-04T09:40:34.051412Z","iopub.status.idle":"2025-02-04T09:40:38.213827Z","shell.execute_reply.started":"2025-02-04T09:40:34.051368Z","shell.execute_reply":"2025-02-04T09:40:38.212758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fastapi import FastAPI, HTTPException\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Set up Hugging Face authentication\nHUGGINGFACE_TOKEN = \"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\"\n\n# Load Llama-3-8B model and tokenizer\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(\n        MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=HUGGINGFACE_TOKEN\n    )\n    sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\nexcept Exception as e:\n    raise RuntimeError(f\"Failed to load model: {str(e)}\")\n\n# Initialize FastAPI\napp = FastAPI()\n\n@app.get(\"/\")\ndef home():\n    return {\"message\": \"Llama-3 Sentiment Analysis API is running!\"}\n\n@app.post(\"/analyze/\")\ndef analyze_sentiment(text: str):\n    try:\n        result = sentiment_pipeline(text)\n        return {\"text\": text, \"sentiment\": result}\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=str(e))\n\n# Run with: uvicorn fastapi_llama3:app --host 0.0.0.0 --port 8000\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T09:55:13.588203Z","iopub.execute_input":"2025-02-04T09:55:13.588552Z","iopub.status.idle":"2025-02-04T10:02:39.700416Z","shell.execute_reply.started":"2025-02-04T09:55:13.588515Z","shell.execute_reply":"2025-02-04T10:02:39.699944Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Kaggle doesn't support fastAPI running directly and I don't have any runtime on collab. So I cannot run the API. Here is alternative-","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Set up Hugging Face authentication\nHUGGINGFACE_TOKEN = \"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\"\n\n# Load Llama-3-8B model and tokenizer\nMODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n\n# Load model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    MODEL_NAME, use_auth_token=HUGGINGFACE_TOKEN\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=HUGGINGFACE_TOKEN\n)\n\n# Load sentiment analysis pipeline\nsentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\n# Example input text\ntext = \"This is a great day!\"\n\n# Get the sentiment of the text\nresult = sentiment_pipeline(text)\n\n# Print the result\nprint(\"Sentiment Analysis Result:\", result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T10:10:28.976411Z","iopub.execute_input":"2025-02-04T10:10:28.976801Z","iopub.status.idle":"2025-02-04T10:11:43.418708Z","shell.execute_reply.started":"2025-02-04T10:10:28.976770Z","shell.execute_reply":"2025-02-04T10:11:43.417692Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Step 7: Load Models","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n# Load the fine-tuned model and tokenizer from Hugging Face\nmodel_name = \"Krati132/distilbert_IMDB\"\n\ntry:\n    # Load model and tokenizer\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    print(\"Model loaded successfully\")\nexcept Exception as e:\n    print(f\"Failed to load model: {str(e)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T10:13:03.953740Z","iopub.execute_input":"2025-02-04T10:13:03.954153Z","iopub.status.idle":"2025-02-04T10:13:12.929754Z","shell.execute_reply.started":"2025-02-04T10:13:03.954121Z","shell.execute_reply":"2025-02-04T10:13:12.929195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install groq requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:10:06.836130Z","iopub.execute_input":"2025-02-04T11:10:06.836464Z","iopub.status.idle":"2025-02-04T11:10:10.729994Z","shell.execute_reply.started":"2025-02-04T11:10:06.836433Z","shell.execute_reply":"2025-02-04T11:10:10.728984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\n\n# Define the API endpoint and headers\nurl = \"https://api.groq.com/openai/v1/chat/completions\"\nheaders = {\n    \"Authorization\": f\"Bearer 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\",\n    \"Content-Type\": \"application/json\"\n}\n\n# Define the payload (input data for the model)\npayload = {\n    \"model\": \"llama-guard-3-8b\",  # Specify the model name\n    \"messages\": [\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ],\n    \"max_tokens\": 50\n}\n\n# Send the request\nresponse = requests.post(url, headers=headers, json=payload)\n\n# Check the response\nif response.status_code == 200:\n    result = response.json()\n    generated_text = result.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n    print(\"Generated Text:\", generated_text)\nelse:\n    print(\"Error:\", response.status_code, response.text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-04T11:22:29.093176Z","iopub.execute_input":"2025-02-04T11:22:29.093458Z","iopub.status.idle":"2025-02-04T11:22:29.346767Z","shell.execute_reply.started":"2025-02-04T11:22:29.093438Z","shell.execute_reply":"2025-02-04T11:22:29.346022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is what I've stored in main.py file-\n\nfrom fastapi import FastAPI, Request\nfrom transformers import pipeline\n\n# Initialize the FastAPI app\napp = FastAPI()\n\n# Load the fine-tuned model from Hugging Face\nmodel = pipeline('sentiment-analysis', model=\"Krati132/distilbert_IMDB\")\n\n@app.post(\"/analyze\")\nasync def analyze(request: Request):\n    # Parse the request body\n    request_data = await request.json()\n    text = request_data.get(\"text\", \"\")\n\n    # Use the model to predict sentiment\n    result = model(text)\n    \n    return {\"sentiment\": result}","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\n\n# Replace with your actual ngrok URL or localhost if testing locally\nngrok_url = \"https://f9af-84-249-11-218.ngrok-free.app/analyze\"  # Change this if using ngrok\n\n# Define the payload with the text you want to analyze\npayload = {\n    \"text\": \"I love programming in Python!\"  # Sample text\n}\n\n# Set the headers\nheaders = {\n    \"Content-Type\": \"application/json\"\n}\n\n# Send the POST request\nresponse = requests.post(ngrok_url, headers=headers, data=json.dumps(payload))\n\n# Check if the request was successful\nif response.status_code == 200:\n    print(\"Response from API:\", response.json())\nelse:\n    print(f\"Failed to connect. Status code: {response.status_code}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-08T20:00:12.033432Z","iopub.execute_input":"2025-02-08T20:00:12.033775Z","iopub.status.idle":"2025-02-08T20:00:13.009149Z","shell.execute_reply.started":"2025-02-08T20:00:12.033749Z","shell.execute_reply":"2025-02-08T20:00:13.008325Z"}},"outputs":[{"name":"stdout","text":"Response from API: {'result': 'Success'}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"I cannot use Google collab and I need GPU. So I used Ngrok for URL and save main.py which is running fast API methods. Its still taking too long to respond. Same with postman. I set the method the 'post' and gave it my ngrok url. And the wrote the foolwoing script-\n{\n    \"model\": \"llama-3.3-70b-versatile\",\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What is the capital of France\"\n        }\n    ]\n}\n\nHere is the output-\nCloud Agent Error: Request is taking longer than 30 seconds to fulfill. Use another agent to send a request without time limitations.\n","metadata":{}},{"cell_type":"markdown","source":"Link to Git: https://github.com/kratipandya/Assignment3","metadata":{}}]}